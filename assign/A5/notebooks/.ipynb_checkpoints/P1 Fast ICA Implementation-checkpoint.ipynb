{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## William Koehrsen wjk68\n",
    "\n",
    "In this notebook, we look at implementing independent component analysis (ICA). This method can be used for blind source separation, or in other words, to determine the independent individual signals comprising a single mixed signal. Applications of ICA include separating out the individual parts of audio signals or dimensionality reduction for classification/regression machine learning models. For example, if there are 3 instruments playing at the same time, ICA can be used to separate out the individual instruments from the composite signal. The components returned by ICA are maximally independent, which means that information on one component does not give information on the other components. ICA operates under the assumption that the components are independent and non-Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for linear algebra \n",
    "from scipy import linalg\n",
    "\n",
    "# Convergence check\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of FastICA\n",
    "\n",
    "The particular implementation of Independent Component Analysis we will use here is known as fast ica. Reference for this method can be found in [the paper](https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf) \"Independent Component Analysis:\n",
    "Algorithms and Applications\" by Aapo Hyvärinen and Erkki Oja. \n",
    "\n",
    "Additional information on ICA used in the report can be found in [this book](https://www.cs.helsinki.fi/u/ahyvarin/papers/bookfinal_ICA.pdf). In particular, the algorithm is developed using information presented in Chapter 8 (pages 165-199). \n",
    "\n",
    "A third source consulted for this method was [the paper](http://www.ccs.neu.edu/home/jaa/CS6800.11F/Topics/Papers/Hyvarinen97.pdf) \"Independent Component Analysis by Minimization of Mutual Information\" by Aapo Hyvärinen. \n",
    "\n",
    "ICA assumes the data matrix, X, to be a linear combination of non-Gaussian (independent) components. In other words, $X = \\text{mixing} * \\text{source}$ where mixing and source are both matrices (this equation is generally written as $X = AS$). The columns of the source matrix contain the independent components, and A is the linear mixing matrix. The objective of ICA is to un-mix the data by finding the un-mixing matrix (W). The source matrix, S, can then be found from $S = W K X$ where K is a pre-whitening matrix that projects the data matrix onto the principal components. \n",
    "\n",
    "When running ICA, we need to specify a number of components, `n_components`. The shape of the matrices are:\n",
    "\n",
    "* X, data matrix: [`n_samples, n_features`] \n",
    "* S, source matrix: [`n_samples, n_components`]\n",
    "* W, un-mixing matrix: [`n_components, n_features`]\n",
    "* K, whitening matrix: [`n_components, n_features`]\n",
    "\n",
    "Once the S, W, and K matrices have been found, the mixing matrix can be calculated from \n",
    "$A = WK^-1$ where the shape is [`n_features, n_components`]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function: Negentropy\n",
    "\n",
    "The cost function used in ICA is the negative entropy, negentropy. Negative entropy is a measure of non-Gaussianity that is based on the information-theroy quantity of entropy. The more unpredictable a variable, the greater its entropy.  A Gaussian random variable has maximum entropy. Rather than using entropy, we will use a measure called negentropy, which is always non-negative and zero if and only if the variable has a Gaussian distribution. Therefore, if the utility function is Negentropy, the goal is to maximize this quantity. The definition of Negentropy is:\n",
    "\n",
    "$$J(y) = H(y_{gauss}) - H(y)$$\n",
    "\n",
    "where $y_{gauss}$ is a random Gaussian varialbe with the same covariance matrix as $y$. \n",
    "\n",
    "This follows from the definition of Entropy for a continuous variable:\n",
    "\n",
    "$$H(y) = -\\int{f(y)\\log{f(y)}d(y)}$$\n",
    "\n",
    "Negentropy is extremely computationally expensive to calculate because it requires estimation of the probability density function (pdf). Therefore approximations of Negentropy are used in practice. In this notebook we will be using the following approximation.\n",
    "\n",
    "![](../images/negentropy_approximation.png)\n",
    "\n",
    "Where $v$ is a Guassian variable of zero mean and unit variance, $k_i$ are positive constants, the variable $y$ has zero mean and unit variance, and $G_i$ are nonquadratic functions. We will use a single non-quadratic function. Good choices for $G$ are functions that do not grow too fast. \n",
    "\n",
    "The following is the log cosh function that we will be using for the non-quadratic function:\n",
    "\n",
    "$$G_1(u) = \\frac{1}{a_1}\\log{\\cosh{a_1u}}$$\n",
    "\n",
    "Here $1 \\le{a_1}\\le{2}$ and the overall method gives a good approximation of Negentropy. \n",
    "\n",
    "We can use the identity \n",
    "\n",
    "$$\\frac{d}{dx}(\\log{\\cosh{x}}) = \\tanh{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate g and g_ for a function\n",
    "# Standard non_linear function\n",
    "def logcosh(x, alpha = 1.0):\n",
    "\n",
    "    x *= alpha\n",
    "    \n",
    "    # Apply tanh inplace\n",
    "    # Using the identity that the deri\n",
    "    gx = np.tanh(x, x)\n",
    "    g_x = np.empty(x.shape[0])\n",
    "    \n",
    "    for i, gx_i in enumerate(gx):\n",
    "        g_x[i] = (alpha * (1 - gx_i ** 2)).mean()\n",
    "        \n",
    "    return gx, g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Centering\n",
    "\n",
    "This simply entails subtracting the mean from each feature of a sample. The resulting variable is a zero-mean variable. The mean can be added back into the centered sources after the mixing matrix has been estimated with the centered data. Also, to re-construct the original signal, we need to add the mean back into the product of the mixing and source matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening Components\n",
    "\n",
    "The observed variables must be whitened after they are centered. Whitening results in a new vector that has uncorrelated coponents with variances equal to unity. The result is that the covariance matix of the vector quals the identity matrix. \n",
    "\n",
    "Here we will use the eigenvalue decomposition of the convariance matrix to whiten the components. The whitening process reudcues the number of parameters that need to be estaimed because the new mixing matrix is orthogonal. This reduces the degrees of freedom and reduces the complexity of the independent component analysis problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and Whiten components\n",
    "def whiten_components(X, n_components):\n",
    "    \n",
    "    n, p = X.shape\n",
    "\n",
    "    X_mean = X.mean(axis=-1)\n",
    "    \n",
    "    # Subtract the mean for 0 mean\n",
    "    X -= X_mean[:, np.newaxis]\n",
    "    \n",
    "    # Preprocessing by PCA\n",
    "    u, d, _ = linalg.svd(X, full_matrices=False)\n",
    "    \n",
    "    # Whitening matrix\n",
    "    whitening = (u / d).T[:n_components]\n",
    "    \n",
    "    # Project data onto the principal components using the whitening matrix\n",
    "    X1 = np.dot(whitening, X)\n",
    "    X1 *= np.sqrt(p)\n",
    "    \n",
    "    # Return whitened components, whitening matrix, and mean of components\n",
    "    return X1, whitening, X_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Decorrelation\n",
    "\n",
    "The outputs must be decorrelated to prevent the vectors from converging to the same maxima. Decorrelation occurs after every iteration. There are several methods of decorrelation including deflation, where the indepedent components are estimated one-by-one, and symmetric where all the components are estimated at once. This means that no vectors are privileged over any others. \n",
    "\n",
    "Symmetric decorrelation is expressed in the following equation:\n",
    "\n",
    "$$W = (WW^T)^{-\\frac{1}{2}}W$$\n",
    "\n",
    "Where $W$ is the matrix of the vecots and the inverse square root is found from the eigenvalue decomposition. There are also iterative algorithms to determine the symmetric decorrelation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric decorrelation of un_mixing matrix\n",
    "# Ensures no vectors are privileged over others\n",
    "def symmetric_decorrelation(un_mixing):\n",
    "    \n",
    "    # Find eigenvalues and eigenvectors of unmixing matrix\n",
    "    eig_values, eig_vectors = linalg.eigh(np.dot(un_mixing, un_mixing.T))\n",
    "    \n",
    "    # Apply symmetric decorrelation equation\n",
    "    sym_un_mixing = np.dot(np.dot(eig_vectors * (1 / np.sqrt(eig_values)), eig_vectors.T), un_mixing)\n",
    "    \n",
    "    return sym_un_mixing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel ICA\n",
    "\n",
    "The parallel ica algorithm estimates all of the independent components at once on each iteration. This requires using the symmetric decorrelation on each iteration After estimating the components on one iteration, the Negentropy is calculated. If the Negentropy increase is below the tolerance, the algorithm stops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_ica(X, init_un_mixing, alpha = 1.0, max_iter = 1000, tol = 1e-4, return_iter = False):\n",
    "    \n",
    "    # Symmetric decorrelation of initial un-mixing components \n",
    "    un_mixing = symmetric_decorrelation(init_un_mixing)\n",
    "    \n",
    "    \n",
    "    p = float(X.shape[1])\n",
    "    \n",
    "    # Iteratively update the un-mixing matrix\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # Function (g) and gradient from the Negentropy approximation\n",
    "        gwtx, g_wtx = logcosh(np.dot(un_mixing, X), alpha)\n",
    "        \n",
    "        # Decorrelate the un-mixing matrix on every loop\n",
    "        new_un_mixing = symmetric_decorrelation(np.dot(gwtx, X.T) / p - g_wtx[:, np.newaxis] * un_mixing)\n",
    "        \n",
    "        # Calculate convergence measure \n",
    "        lim = max(abs(abs(np.diag(np.dot(new_un_mixing, un_mixing.T))) - 1))\n",
    "        \n",
    "        # Update un-mixing \n",
    "        un_mixing = new_un_mixing\n",
    "\n",
    "        # Check for convergence\n",
    "        if lim < tol:\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        warnings.warn('FastICA algorithm did not converge. Considering increasing '\n",
    "                      'tolerance or increasing the maximum number of iterations.')\n",
    "       \n",
    "    # Return the un-mixing matrix\n",
    "    if return_iter:\n",
    "        return un_mixing, i + 1\n",
    "    else: \n",
    "        return un_mixing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast ICA\n",
    "\n",
    "There are several desirable properties of the Fast ICA algorithm compared to the traditional methods for ICA.\n",
    "\n",
    "1. The convergence is cubic (or at the least quadratic) under the ICA data model assumptions. In contrast, stochastic gradient descent methods only converge linearly. \n",
    "2. There is no step-size parameter to select as in gradient-based methods. This increases usability of the method.\n",
    "3. The Fast ICA algorithm direclty finds independent components of nearly any non-Gaussian distribution.\n",
    "4. The method can be optimized using the right non-linearity, g.\n",
    "5. Implemented is parallel and can be distributed. \n",
    "\n",
    "First, the input data must be centered and whitened. Then, the initial un-mixing components are randomly intialized. The algorithm then updates the un-mixing components until covergence. Finally, the mixing matrix and sources can be estimated from the un-mixing matrix and the whitening matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = mixing * sources\n",
    "# sources = un-mixing * whitening * X\n",
    "def perform_fastica(X, n_components, alpha = 1.0, max_iter = 200, tol = 1e-4):\n",
    "    \n",
    "    # Center and Whiten components\n",
    "    X1, whitening, X_mean = whiten_components(X.T, n_components)\n",
    "    \n",
    "    # initial un_mixing components\n",
    "    init_un_mixing = np.asarray(np.random.normal(size = (n_components, n_components)))\n",
    "    \n",
    "    # Solve ica using the parallel ica algorithm\n",
    "    un_mixing = parallel_ica(X1, init_un_mixing, alpha, max_iter, tol)\n",
    "\n",
    "    # Calculate the sources\n",
    "    sources = np.dot(np.dot(un_mixing, whitening), X.T).T\n",
    "    \n",
    "    # Calculate the mixing matrix\n",
    "    w = np.dot(un_mixing, whitening)\n",
    "    mixing = linalg.pinv(w)\n",
    "    \n",
    "    # Return mixing matrix, sources, and mean of X\n",
    "    return mixing, sources, X_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse ICA Transform\n",
    "\n",
    "The inverse transform can be used to re-construct the original matrix from the estimated sources and mixing matrix. The mean must also be added back in to find the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_fastica(mixing, source, X_mean):\n",
    "    # Inverse transform\n",
    "    X = np.dot(sources, mixing.T)\n",
    "    # Add back in mean\n",
    "    X += X_mean\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we implemented Independent Component Analysis, a method of blind source separation that finds the independent components assuming that a signal is a linear combination of non-Gaussian sources. We used the FastICA implementation of independent component analysis because of its advantages relative to gradient-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
