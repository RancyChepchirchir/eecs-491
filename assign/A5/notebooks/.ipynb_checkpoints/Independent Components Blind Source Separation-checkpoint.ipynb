{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import signal, linalg\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of FastICA\n",
    "\n",
    "http://www.ccs.neu.edu/home/jaa/CS6800.11F/Topics/Papers/Hyvarinen97.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "features = digits.images.reshape(-1, 64)\n",
    "labels = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (negative entropy)\n",
    "\n",
    "Using log cos approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening Components (using SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel ICA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logcosh(x, alpha = 1.0):\n",
    "\n",
    "    x *= alpha\n",
    "    \n",
    "    gx = np.tanh(x, x)\n",
    "    g_x = np.empty(x.shape[0])\n",
    "    \n",
    "    for i, gx_i in enumerate(gx):\n",
    "        g_x[i] = (alpha * (1 - gx_i ** 2)).mean()\n",
    "        \n",
    "    return gx, g_x\n",
    "\n",
    "# Whiten and pre-process components\n",
    "def whiten_pca_components(X, n_components):\n",
    "    \n",
    "    n, p = X.shape\n",
    "    \n",
    "    X_mean = X.mean(axis=-1)\n",
    "    \n",
    "    # Subtract the mean for 0 mean\n",
    "    X -= X_mean[:, np.newaxis]\n",
    "    \n",
    "    # Preprocessing by PCA\n",
    "    u, d, _ = linalg.svd(X, full_matrices=False)\n",
    "    \n",
    "    # Whitening matrix\n",
    "    whitening = (u / d).T[:n_components]\n",
    "    \n",
    "    # Project data onto the principal components using the whitening matrix\n",
    "    X1 = np.dot(whitening, X)\n",
    "    X1 *= np.sqrt(p)\n",
    "    \n",
    "    # Return whitened components, whitening matrix, and mean of components\n",
    "    return X1, whitening, X_mean\n",
    "\n",
    "# Symmetric decorrelation of un_mixing matrix\n",
    "# https://ieeexplore.ieee.org/document/398721/\n",
    "# Ensures no vectors are privileged over others\n",
    "def symmetric_decorrelation(un_mixing):\n",
    "    \n",
    "    # Find eigenvalues and eigenvectors of initial weight matrix\n",
    "    eig_values, eig_vectors = linalg.eigh(np.dot(un_mixing, un_mixing.T))\n",
    "    # Symmetric decorrelation equation\n",
    "    sym_un_mixing = np.dot(np.dot(eig_vectors * (1 / np.sqrt(eig_values)), eig_vectors.T), un_mixing)\n",
    "    \n",
    "    return sym_un_mixing\n",
    "\n",
    "\n",
    "def parallel_ica(X, init_un_mixing, alpha = 1.0, max_iter = 1000, tol = 1e-4, return_iter = False):\n",
    "    \n",
    "    # Symmetric decorrelation of initial un-mixing components \n",
    "    un_mixing = symmetric_decorrelation(init_un_mixing)\n",
    "    \n",
    "    \n",
    "    p = float(X.shape[1])\n",
    "    \n",
    "    # Iteratively update the un-mixing matrix\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # Function and derivative \n",
    "        gwtx, g_wtx = logcosh(np.dot(un_mixing, X), alpha)\n",
    "        \n",
    "        \n",
    "        new_un_mixing = symmetric_decorrelation(np.dot(gwtx, X.T) / p - g_wtx[:, np.newaxis] * un_mixing)\n",
    "        \n",
    "        # Calculate negative entropy based on logcosh\n",
    "        lim = max(abs(abs(np.diag(np.dot(new_un_mixing, un_mixing.T))) - 1))\n",
    "        \n",
    "        # Update un-mixing \n",
    "        un_mixing = new_un_mixing\n",
    "        print(un_mixing.shape)\n",
    "        # Check for convergence\n",
    "        if lim < tol:\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        warnings.warn('FastICA algorithm did not converge. Considering increasing '\n",
    "                      'tolerance or increasing the maximum number of iterations.')\n",
    "        \n",
    "    if return_iter:\n",
    "        return un_mixing, i + 1\n",
    "    else: \n",
    "        return un_mixing\n",
    "    \n",
    "# X = mixing * sources\n",
    "# sources = un-mixing * whitening * X\n",
    "def perform_fastica(X, n_components, alpha = 1.0, max_iter = 200, tol = 1e-4):\n",
    "    \n",
    "    X1 = X.T\n",
    "    # Whiten components by subtracting mean\n",
    "    X1, whitening, X_mean = whiten_pca_components(X1, n_components)\n",
    "    \n",
    "    # initial un_mixing components\n",
    "    init_un_mixing = np.asarray(np.random.normal(size = (n_components, n_components)))\n",
    "    \n",
    "    # Solve ica using the parallel ica algorithm\n",
    "    un_mixing = parallel_ica(X1, init_un_mixing, alpha, max_iter, tol)\n",
    "\n",
    "    # Calculate the sources\n",
    "    sources = np.dot(np.dot(un_mixing, whitening), X.T).T\n",
    "    \n",
    "    # Calculate the mixing matrix\n",
    "    w = np.dot(un_mixing, whitening)\n",
    "    mixing = linalg.pinv(w)\n",
    "    \n",
    "    # Return mixing matrix, sources, and mean of X\n",
    "    return mixing, sources, X_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving fastica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse ICA Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_fastica(mixing, source, X_mean):\n",
    "    # Inverse transform\n",
    "    X = np.dot(sources, mixing.T)\n",
    "    # Add back in mean\n",
    "    X += X_mean\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
